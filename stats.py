from flask import *  
import sqlite3
from datetime import datetime
import os 
import pandas as pd
import sklearn 
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import csv  
from flask import jsonify
import shutil
import os
import stat
import ppscore as pps


#Inserts data in the database with the autogenerated id and other values related to popup are none
#date is the date when the data has inserted it is given from frontend

def insertdata(modelName,modelDesc,user,modelFile,date_time): #testing done
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""INSERT INTO Models(modelName,modelDesc,user,modelFile,date) 
		VALUES
		(?,?,?,?,?)""",(modelName,modelDesc,user,modelFile,date_time))
	con.commit()
	id = c.execute("select id from Models where id = (select max(id) from Models)")
	id = c.fetchall()[0][0]
	con.close()
	path = "static/"+str(id)
	os.mkdir(path)
	path = "static/"+str(id)+"/temp"
	os.mkdir(path)
	return(id)

#NOTE____________features shound be the in the  , seperated string 
def updatedata(features,Score, status,alerts,bunessAction,escalation,id):
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""UPDATE Models SET features = ?,
	Score = ?,
	status = ?,
	alerts = ?,
	bunessAction = ?,
	escalation =? WHERE id = ?""",(features,Score, status,alerts,bunessAction,escalation,id))
	con.commit()
	con.close()


def updatefile(file,id):
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""UPDATE Models SET modelFile = ? WHERE id = ?""",(file,id))
	con.commit()
	con.close()

#returns whole data of id from database
def get_data_by_id(id):
	con = sqlite3.connect("Model.db")
	c = con.cursor() 
	c.execute("SELECT * FROM Models WHERE id = ?", (id,))
	data = c.fetchall()[0]
	con.close()
	return(data)

#returns all the rows from database
def get_all_rows():
     with sqlite3.connect("Model.db") as con:
            cur = con.cursor()
            con.row_factory = sqlite3.Row
            cur.execute("select * from Models")
            rows = cur.fetchall()
            return(rows)

#to delete the model  and the folder associated with the model
def delete_model(id):
	path = "static/" +str(id)
	os.chmod(path, 0o777)
	shutil.rmtree(path, ignore_errors=True)
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("DELETE FROM Models where id = ?", (id,))
	con.commit()
	con.close()


#function to convert string to list of featutes

def feature_list(features):
	features.replace(" ", "")
	features = features.split(',')
	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
	features = up_feature
	return(features) 


#mean, Q1, median, Q3, interQuantileRange, min, max,STD = statistics_of_feature(df[columns[0]])//data for value of column
#and returns data in below order
#reurns mean, Q1, median, Q3, interQuantileRange, min, max,STD,MAD
#flag =1 is for alerted
#flag =2 is for non-alerted
#flag = 3 is for escalations
def statistics_of_all_feature(id,flag, outputfile):
	data = get_data_by_id(id)
	features =feature_list(data[6])
	file = data[4]
	
	alert_flag = data[9]
	df = pd.read_csv(file)
	escalation_flag= data[11]
	if (flag == 1):
		df = df[df[alert_flag] == 1]
	elif (flag == 2):
		df = df[df[alert_flag] == 0]
	else:
		df = df[df[escalation_flag] == 1]

	calculate_stats(df,features,outputfile);
	

#function to calculate statistics on besis of dataframe list of features and result is stored in the outputfile
def calculate_stats(df,features,outputfile):
	f= open(outputfile,"w+")
	f.write("Feature_name,Mean, Q1, Median, Q3, InterQuantileRange, Min, Max,STD,MAD\n")
	for fname in features:
		data = df[fname]
		data = pd.DataFrame(data)
		if(data.empty):
			mean = q1 = median = q3 = interQuantileRange =min =	max = 	STD = 	MAD = 0.0
			f.write(fname + "," +str(mean)+","+ str(q1) + "," + str(median) + "," +str(q3) + "," + str(interQuantileRange) + "," +str(min) + "," + str(max) + "," + str(STD) + "," +str(MAD)+"\n")
		else:
			mean = data.mean()
			q1 = data.quantile(0.25)
			median = data.quantile(0.5)
			q3 = data.quantile(0.75)
			interQuantileRange = q3 - q1
			min = data.min()
			max = data.max()
			STD = data.std()
			MAD = data.mad()
			f.write(fname + "," +str(round(mean[0],3))+","+ str(round(q1[0],3)) + "," + str(round(median[0],3)) + "," +str(round(q3[0],3)) + "," + str(round(interQuantileRange[0],3)) + "," +str(round(min[0],3)) + "," + str(round(max[0],3)) + "," + str(round(STD[0],3)) + "," +str(round(MAD[0],3))+"\n")
	f.close()

#Number of total cases per surveillance scenario
#Number of Alerts per surveillance scenario
#non alerts per surveillance scenari
#Number of Escalations per surveillance scenario
#Number of Alerts vs Overall
def common_calculations(id):#testing done
	#total cases per surveillance
	data = get_data_by_id(id)
	alert_flag = data[9]
	escalation_flag= data[11]
	file = data[4]
	df = pd.read_csv(file)
	total_cases_surv = len(df.index)
	alert_per_surv =df[df[alert_flag] == 1].shape[0]
	nonalert_per_surv =df[df[alert_flag] == 0].shape[0]
	escalations_per_surv = df[df[escalation_flag] == 1].shape[0]
	return (total_cases_surv,alert_per_surv,nonalert_per_surv,escalations_per_surv)

#Number of Alerts per business reviews
#It returns business review and their frequncies respectively
def alerts_per_business_review(id, outputfile):#testing done
	data = get_data_by_id(id)
	status = data[8]
	alert_flag = data[9]
	file = data[4]
	df = pd.read_csv(file)
	df = df[df[alert_flag] == 1]
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status] != "BLANKS"]

	business_reviews = df[status].unique() 
	alerts_per_business_review = [] 
	business_review = []
	frequency = []
	i = 0
	for review in business_reviews:

	    alerts_per_business_review.append([business_reviews[i],df[df[status] == business_reviews[i]].shape[0]])
	    business_review.append(business_reviews[i])
	    frequency.append(df[df[status] == business_reviews[i]].shape[0])
	    i = i+1
	records = zip(business_review,frequency)
	f= open(outputfile,"w+")
	f.write("review,frequency\n")
	count	= 0
	for i in records:
		abc,xyx =i
		f.write(str(abc)+","+str(xyx)+"\n")
		count +=1
	f.close()
	reviews = list(zip(business_review,frequency))
	return(reviews,count)

#this function returns the feature importance
#flag = 1 is has_alerted
#flag = 0 for escalations
def feature_imp(id,flag,outputfile):#testing done
	data = get_data_by_id(id)
	file = data[4]
	status = data[8]
	dataset = pd.read_csv(file)
	if (flag == 1):
		y_flag = data[9]
	else:
		y_flag = data[11]
		
	features = data[6]
	features.replace(" ", "")
	features = features.split(',')

	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
	features = up_feature

	
	X = dataset.loc[:, dataset.columns.isin(features)].values
	y = dataset.iloc[:, dataset.columns.get_loc(y_flag)].values #for the has alerted column 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
	#test_size =0.2 shows that the 20% data is used for testing and 80% for training
	#random state = 0 means it keeps same result for again and again executions
	sc = StandardScaler()
	X_train = sc.fit_transform(X_train)
	X_test = sc.transform(X_test)
	model = RandomForestClassifier(n_estimators=20, random_state=0)
	#estimators show the number of random trees
	model.fit(X_train, y_train) # training model
	importances = model.feature_importances_
	feature_importances_matrix = list(zip(features,importances))
	
	f= open(outputfile,"w+")
	f.write("features,importances\n")
	for i in feature_importances_matrix:
		abc, xyx=i
		
		xyx = xyx*100
		f.write(str(abc)+","+str(xyx)+"\n")
	f.close()
	return(feature_importances_matrix)


#correlation matrix
#flag =1 is pearson 
#flag = 0 is spearman
def correlation_mat(id, flag,outputfile): #testing done
	data = get_data_by_id(id)
	file = data[4]
	features =feature_list(data[6])
	if (flag == 1):
		mthd = "pearson"
	else:
		mthd = "spearman"
	data = pd.read_csv(file)
	df = pd.DataFrame(data,columns = features)
	corrMatrix = df.corr(method=mthd)
	res = corrMatrix.to_numpy()
	res
	f = open(outputfile,"w+")
	f.write("group,variable,value\n")
	i = 0
	j =0
	for f1 in features:
	    
	    for f2 in features:
	        f2 = "feature"+str(j+1)
	        value = res[i][j]
	        j = j + 1
	        f.write(str(f1) + "," + str(f2) + "," + str(value)+ "\n")
	    i = i+1
	    j= 0
	f.close()
	#return corrMatrix


#returns confusion matrix,precision, recall, accuracy 
def confusion_metrix(id):#testing done
	data = get_data_by_id(id)
	file = data[4]
	alert_flag = data[9]
	escalation_flag = data[11]
	df = pd.read_csv(file)

	df = df[df[alert_flag] == 1]
	df = df[df[escalation_flag] == 1]
	tp = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 0]
	df = df[df[escalation_flag] == 1]
	fn = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 1]
	df = df[df[escalation_flag] == 0]
	fp = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 0]
	df = df[df[escalation_flag] == 0]
	tn = len(df.index)
	total = tp+fp+fn+tn
	matrix = [tp,fp,fn,tn]
	accuracy = (tp+tn) /total
	precision = (tp) /(tp+fp)
	recall = tp /(tp+fn)
	accuracy = accuracy*100
	precision = precision*100
	recall = recall*100
	return(tp,fp,fn,tn,accuracy,precision,recall)
	

#for each business review it creates book(serial number of review).csv 
#output file contains the data required for the boxplot
def business_review_boxplot(id):
	data = get_data_by_id(id)
	status = data[8]
	features =feature_list(data[6])
	file = data[4]
	df = pd.read_csv(file)
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status] != "BLANKS"]
	business_reviews = df[status].unique()	
	i = 0
	for review in business_reviews:
		outputfile = "static/"+str(id)+"/temp/"+ "book"+str(i)+".csv"
		f = open(outputfile,"w+")
		f.write("feature_name,mean,q1,median,q3,interQuantileRange,min,max\n")
		temp = df[df[status] == review]
		box_stat = boxplot_stat(temp,features,f)
	
		f.close()
		i+=1
	return	(business_reviews)
	
#writes into file the statistics required for boxplot
def boxplot_stat(df, features,f):
	statistics = []
	for fname in features:
		data = df[fname]
		data = pd.DataFrame(data)
		mean = data.mean()
		q1 = data.quantile(0.25)
		median = data.quantile(0.5)
		q3 = data.quantile(0.75)
		interQuantileRange = q3 - q1
		min = data.min()
		max = data.max()
		box_stat = (fname+ "," +str(mean[0])+","+ str(q1[0]) + "," + str(median[0]) + "," +str(q3[0]) + "," + str(interQuantileRange[0]) + "," +str(min[0]) + "," + str(max[0]) + "\n")
		f.write(box_stat)
	return(statistics)


#returns strip(serial number of review).csv which contains required data of related review to plot strip plot  
def violin_business_review(id):
	data = get_data_by_id(id)
	status = data[8]
	features =feature_list(data[6])
	file = data[4]
	
	df = pd.read_csv(file)
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status]!= "BLANKS"]
	business_reviews = df[status].unique()

	j =0
	for rev in  business_reviews:
		
		temp = df[df[status] == rev]
		filename = "static/"+str(id)+"/temp/"+"strip"+str(j)+".csv"
		f = open(filename,"w+")
		f.write("feature,value\n")

		for feature in features:
			newtemp = temp[feature].to_numpy()
			for i in newtemp:
				f.write(feature + "," + str(i) + "\n")

		f.close()
		j +=1


#returns the column headers required to show in popup
def get_column_header(id):
	data = get_data_by_id(id)
	file = data[4]
	score = data[7]
	df = pd.read_csv(file)
	columns = list(df.columns)
	return(columns)


#returns feature count for  model with id 
def get_feature_count(id):
	data = get_data_by_id(id)
	features = data[6]
	features.replace(" ", "")
	features = features.split(',')
	count = 0
	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
			count +=1
	features = list(up_feature)
	return (count)









#function created the cutoff related data
def cutoff_file(id,cutoff):#testing done
	data = get_data_by_id(id)
	score = data[7]
	file = data[4]
	escalation_flag= data[11]
	features =feature_list(data[6])
	alert_flag = data[9]
	df = pd.read_csv(file)

	for i in df.index:
		if df.at[i,score] < cutoff:
		    df.at[i, alert_flag] = 0
		else:
		    df.at[i, alert_flag] = 1

	arr = file.split(".")
	arr = arr[0].split("/")
	name  = "static/"+str(id)+"/temp/" + arr[-1] +"cutoff.csv"
	df.to_csv(name)
	alerted = df[df[alert_flag] == 1]
	escalated = alerted[alerted[escalation_flag] == 1]
	non_alerted = df[df[alert_flag] == 0]
	calculate_stats(alerted,features,"static/" + str(id) + "/temp/statistics_of_alert_feature_cutoff" + str(cutoff) + ".csv")
	calculate_stats(non_alerted,features,"static/" + str(id) + "/temp/statistics_of_nonalert_feature_cutoff" + str(cutoff) + ".csv")
	calculate_stats(escalated,features,"static/" + str(id) + "/temp/statistics_of_escalated_feature_cutoff" + str(cutoff) + ".csv")
	return(len(alerted.index), len(non_alerted.index),len(escalated.index))


def cutofffinal(id):
	outputfile = "static/"+str(id)+"/temp/cutoff.csv"
	f = open(outputfile,"w+")
	f.write("cut_off,alerts,nonalerts,escalations\n")
	for i in range(10,100,10):
		alerts,nonalerts,escalations = cutoff_file(id, i)
		f.write(str(i) + ","+str(alerts)+ "," + str(nonalerts)+"," + str(escalations) + "\n")
	f.close()


def datatoshow(id):
	data =  get_data_by_id(id)
	file = data[4]
	df= pd.read_csv(file)
	df = df.head(50)
	cols  = df.columns
	for i in df.index:
		for col in cols:
			df.at[i,col] = round(df.at[i,col],3)
	df.to_csv("static/"+str(id)+"/temp/data50.csv")


def predictive_score(id,outputfile):
	data = get_data_by_id(id)
	file = data[4]
	features =feature_list(data[6])
	data = pd.read_csv(file)
	df = pd.DataFrame(data,columns = features)
	mat = pps.matrix(df[features])
	res = mat.to_numpy()
	res
	f = open(outputfile,"w+")
	f.write("group,variable,value\n")
	i = 0
	j =0
	for f1 in features:
	    
	    for f2 in features:
	        f2 = "feature"+str(j+1)
	        value = res[i][j]
	        j = j + 1
	        f.write(str(f1) + "," + str(f2) + "," + str(value)+ "\n")
	    i = i+1
	    j= 0
	f.close()

