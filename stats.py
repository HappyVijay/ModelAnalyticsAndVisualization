from flask import *  
import sqlite3
from datetime import datetime
import os 
import pandas as pd
import sklearn 
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import csv  
from flask import jsonify
import shutil
import os
import stat


#Inserts data in the database with the autogenerated id and other values related to popup are none
#date is the date when the data has inserted it is given from frontend

def insertdata(modelName,modelDesc,user,modelFile,date_time): #testing done
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""INSERT INTO Models(modelName,modelDesc,user,modelFile,date) 
		VALUES
		(?,?,?,?,?)""",(modelName,modelDesc,user,modelFile,date_time))
	con.commit()
	id = c.execute("select id from Models where id = (select max(id) from Models)")
	id = c.fetchall()[0][0]
	con.close()
	path = "static/"+str(id)
	os.mkdir(path)
	path = "static/"+str(id)+"/temp"
	os.mkdir(path)
	return(id)

#NOTE____________features shound be the in the  , seperated string 
def updatedata(features,Score, status,alerts,bunessAction,escalation,id):
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""UPDATE Models SET features = ?,
	Score = ?,
	status = ?,
	alerts = ?,
	bunessAction = ?,
	escalation =? WHERE id = ?""",(features,Score, status,alerts,bunessAction,escalation,id))
	con.commit()
	con.close()


def updatefile(file,id):
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("""UPDATE Models SET modelFile = ? WHERE id = ?""",(file,id))
	con.commit()
	con.close()

#For the Dashboard
def boxdata(id):
	data = get_data_by_id(id)
	file = data[4]
	features = feature_list(data[6])
	alert_flag = data[9]
	escalation_flag= data[11]
	df = pd.read_csv(file)
	l = ['Escalations','Alerts','NonAlerts']

	final_data = {}
	for i in range(0, 3):
		final_data[l[i]] = []
		if i == 0:
			temp = df[df[escalation_flag] == 1]
		elif i == 1:
			temp = df[df[alert_flag] == 1]
		elif i == 2:
			temp = df[df[alert_flag] == 0]

		for fname in features:
			d = {}
			d['feature_name'] = fname
			data = temp[fname]
			data = pd.DataFrame(data)
			d['mean'] = data.mean()[0]
			d['q1'] = data.quantile(0.25)[0]
			d['median'] = data.quantile(0.5)[0]
			d['q3'] = data.quantile(0.75)[0]
			d['interQuantileRange'] = d['q3'] - d['q1']
			d['min'] = data.min()[0]
			d['max'] = data.max()[0]
			final_data[l[i]].append(d)

	return final_data

def violindata(id):
	data = get_data_by_id(id)
	file = data[4]
	features = feature_list(data[6])
	alert_flag = data[9]
	escalation_flag= data[11]
	df = pd.read_csv(file)
	l = ['Escalations','Alerts','NonAlerts']
	final_data = {}

	for i in range(0, 3):
		final_data[l[i]] = []
		if i == 0:
			temp = df[df[escalation_flag] == 1]
		elif i == 1:
			temp = df[df[alert_flag] == 1]
		elif i == 2:
			temp = df[df[alert_flag] == 0]

		for feature in features:
			newtemp = temp[feature].to_numpy()
			for x in newtemp:
				d = {}
				d['feature'] = feature
				d['value'] = x
				final_data[l[i]].append(d)
	
	return final_data

#returns whole data of id from database
def get_data_by_id(id):
	con = sqlite3.connect("Model.db")
	c = con.cursor() 
	c.execute("SELECT * FROM Models WHERE id = ?", (id,))
	data = c.fetchall()[0]
	con.close()
	return(data)

#returns all the rows from database
def get_all_rows():
     with sqlite3.connect("Model.db") as con:
            cur = con.cursor()
            con.row_factory = sqlite3.Row
            cur.execute("select * from Models")
            rows = cur.fetchall()
            return(rows)

#to delete the model  and the folder associated with the model
def delete_model(id):
	path = "static/" +str(id)
	os.chmod(path, 0o777)
	shutil.rmtree(path, ignore_errors=True)
	con = sqlite3.connect("Model.db")
	c = con.cursor()
	c.execute("DELETE FROM Models where id = ?", (id,))
	con.commit()
	con.close()


#function to convert string to list of featutes

def feature_list(features):
	features.replace(" ", "")
	features = features.split(',')
	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
	features = up_feature
	return(features) 


#mean, Q1, median, Q3, interQuantileRange, min, max,STD = statistics_of_feature(df[columns[0]])//data for value of column
#and returns data in below order
#reurns mean, Q1, median, Q3, interQuantileRange, min, max,STD,MAD
#flag =1 is for alerted
#flag =2 is for non-alerted
#flag = 3 is for escalations
def statistics_of_all_feature(id,flag):
	data = get_data_by_id(id)
	features =feature_list(data[6])
	file = data[4]
	
	alert_flag = data[9]
	df = pd.read_csv(file)
	escalation_flag= data[11]
	if (flag == 1):
		df = df[df[alert_flag] == 1]
	elif (flag == 2):
		df = df[df[alert_flag] == 0]
	else:
		df = df[df[escalation_flag] == 1]

	statdata = calculate_stats(df,features)
	return(statdata)
	

def calculate_stats(df,features):
	statdata = "Feature_name,Mean, Q1, Median, Q3, InterQuantileRange, Min, Max,STD,MAD\n"
	for fname in features:
		data = df[fname]
		data = pd.DataFrame(data)
		if(data.empty):
			mean = q1 = median = q3 = interQuantileRange =min =	max = 	STD = 	MAD = 0.0
			statdata += fname + "," +str(mean)+","+ str(q1) + "," + str(median) + "," +str(q3) + "," + str(interQuantileRange) + "," +str(min) + "," + str(max) + "," + str(STD) + "," +str(MAD)+"\n"
		else:
			mean = data.mean()
			q1 = data.quantile(0.25)
			median = data.quantile(0.5)
			q3 = data.quantile(0.75)
			interQuantileRange = q3 - q1
			min = data.min()
			max = data.max()
			STD = data.std()
			MAD = data.mad()
			statdata +=fname + "," +str(round(mean[0],3))+","+ str(round(q1[0],3)) + "," + str(round(median[0],3)) + "," +str(round(q3[0],3)) + "," + str(round(interQuantileRange[0],3)) + "," +str(round(min[0],3)) + "," + str(round(max[0],3)) + "," + str(round(STD[0],3)) + "," +str(round(MAD[0],3))+"\n"
	return(statdata)

#Number of total cases per surveillance scenario
#Number of Alerts per surveillance scenario
#non alerts per surveillance scenari
#Number of Escalations per surveillance scenario
#Number of Alerts vs Overall
def common_calculations(id):#testing done
	#total cases per surveillance
	data = get_data_by_id(id)
	alert_flag = data[9]
	escalation_flag= data[11]
	file = data[4]
	df = pd.read_csv(file)
	total_cases_surv = len(df.index)
	alert_per_surv =df[df[alert_flag] == 1].shape[0]
	nonalert_per_surv =df[df[alert_flag] == 0].shape[0]
	escalations_per_surv = df[df[escalation_flag] == 1].shape[0]
	return (total_cases_surv,alert_per_surv,nonalert_per_surv,escalations_per_surv)

#Number of Alerts per business reviews
#It returns business review and their frequncies respectively
def alerts_per_business_review(id):#testing done
	data = get_data_by_id(id)
	status = data[8]
	alert_flag = data[9]
	file = data[4]
	df = pd.read_csv(file)
	df = df[df[alert_flag] == 1]
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status] != "BLANKS"]

	business_reviews = df[status].unique() 
	alerts_per_business_review = [] 
	business_review = []
	frequency = []
	i = 0
	for review in business_reviews:

	    alerts_per_business_review.append([business_reviews[i],df[df[status] == business_reviews[i]].shape[0]])
	    business_review.append(business_reviews[i])
	    frequency.append(df[df[status] == business_reviews[i]].shape[0])
	    i = i+1
	records = zip(business_review,frequency)
	# f= open(outputfile,"w+")
	# f.write("review,frequency\n")
	# count	= 0
	# for i in records:
	# 	abc,xyx =i
	# 	f.write(str(abc)+","+str(xyx)+"\n")
	# 	count +=1
	# f.close()
	reviews = list(zip(business_review,frequency))
	return(reviews)

#this function returns the feature importance
#flag = 1 is has_alerted
#flag = 0 for escalations
#this function returns the feature importance
#flag = 1 is has_alerted
#flag = 0 for escalations
def feature_imp(id,flag):#testing done
	data = get_data_by_id(id)
	file = data[4]
	status = data[8]
	dataset = pd.read_csv(file)
	if (flag == 1):
		y_flag = data[9]
	else:
		y_flag = data[11]
		
	features = data[6]
	features.replace(" ", "")
	features = features.split(',')

	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
	features = up_feature

	
	X = dataset.loc[:, dataset.columns.isin(features)].values
	y = dataset.iloc[:, dataset.columns.get_loc(y_flag)].values #for the has alerted column 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
	#test_size =0.2 shows that the 20% data is used for testing and 80% for training
	#random state = 0 means it keeps same result for again and again executions
	sc = StandardScaler()
	X_train = sc.fit_transform(X_train)
	X_test = sc.transform(X_test)
	model = RandomForestClassifier(n_estimators=20, random_state=0)
	#estimators show the number of random trees
	model.fit(X_train, y_train) # training model
	importances = model.feature_importances_
	feature_importances_matrix = list(zip(features,importances))
	
	# f= open(outputfile,"w+")
	# f.write("features,importances\n")
	# for i in feature_importances_matrix:
	# 	abc, xyx=i
		
	# 	xyx = xyx*100
	# 	f.write(str(abc)+","+str(xyx)+"\n")
	# f.close()
	return(feature_importances_matrix)


#correlation matrix
#flag =1 is pearson 
#flag = 0 is spearman
def correlation_mat(id, flag): #testing done
	data = get_data_by_id(id)
	file = data[4]
	features =feature_list(data[6])
	if (flag == 1):
		mthd = "pearson"
	else:
		mthd = "spearman"
	data = pd.read_csv(file)
	df = pd.DataFrame(data,columns = features)
	corrMatrix = df.corr(method=mthd)
	res = corrMatrix.to_numpy()
	res
	# mat  = "group,variable,value\\n"
	# i = 0
	# j =0
	# for f1 in features:
	    
	#     for f2 in features:
	#         f2 = "feature"+str(j+1)
	#         value = res[i][j]
	#         j = j + 1
	#         mat += str(f1) + "," + str(f2) + "," + str(value)+ "\\n"
	#     i = i+1
	#     j= 0
	
	return (res, features)
	#return corrMatrix
#returns confusion matrix,precision, recall, accuracy 
def confusion_metrix(id):#testing done
	data = get_data_by_id(id)
	file = data[4]
	alert_flag = data[9]
	escalation_flag = data[11]
	df = pd.read_csv(file)

	df = df[df[alert_flag] == 1]
	df = df[df[escalation_flag] == 1]
	tp = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 0]
	df = df[df[escalation_flag] == 1]
	fn = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 1]
	df = df[df[escalation_flag] == 0]
	fp = len(df.index)

	df = pd.read_csv(file)
	df = df[df[alert_flag] == 0]
	df = df[df[escalation_flag] == 0]
	tn = len(df.index)
	total = tp+fp+fn+tn
	matrix = [tp,fp,fn,tn]
	accuracy = (tp+tn) /total
	precision = (tp) /(tp+fp)
	recall = tp /(tp+fn)
	accuracy = accuracy*100
	precision = precision*100
	recall = recall*100
	return(tp,fp,fn,tn,accuracy,precision,recall)
	
#for each business review it creates book(serial number of review).csv 
#output file contains the data required for the boxplot
def business_review_boxplot(id):
	data = get_data_by_id(id)
	status = data[8]
	features =feature_list(data[6])
	file = data[4]
	df = pd.read_csv(file)
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status] != "BLANKS"]
	business_reviews = df[status].unique()
	business_reviews = list(business_reviews)	
	i = 0
	box = {}
	for review in business_reviews:
		temp = df[df[status] == review]
		box[review] = []

		for fname in features:
			d = {}
			d['feature_name'] = fname
			data = temp[fname]
			data = pd.DataFrame(data)
			d['mean'] = data.mean()[0]
			d['q1'] = data.quantile(0.25)[0]
			d['median'] = data.quantile(0.5)[0]
			d['q3'] = data.quantile(0.75)[0]
			d['interQuantileRange'] = d['q3'] - d['q1']
			d['min'] = data.min()[0]
			d['max'] = data.max()[0]
			box[review].append(d)
	print(d['mean'])
	return	(business_reviews, box)

#returns strip(serial number of review).csv which contains required data of related review to plot strip plot  
def violin_business_review(id):
	data = get_data_by_id(id)
	status = data[8]
	features =feature_list(data[6])
	file = data[4]
	
	df = pd.read_csv(file)
	df[status] = df[status].fillna('BLANKS')
	df = df[df[status]!= "BLANKS"]
	business_reviews = df[status].unique()
	business_reviews= list(business_reviews)
	j = 0
	strip = {}

	for rev in  business_reviews:
		strip[rev] = []
		temp = df[df[status] == rev]
		# filename = "static/"+str(id)+"/temp/"+"strip"+str(j)+".csv"
		# f = open(filename,"w+")
		# f.write("feature,value\n")

		for feature in features:
			newtemp = temp[feature].to_numpy()
			for i in newtemp:
				d = {}
				d['feature'] = feature
				d['value'] = i
				strip[rev].append(d)
				
	return strip


#returns the column headers required to show in popup
def get_column_header(id):
	data = get_data_by_id(id)
	file = data[4]
	score = data[7]
	df = pd.read_csv(file)
	columns = list(df.columns)
	return(columns)


#returns feature count for  model with id 
def get_feature_count(id):
	data = get_data_by_id(id)
	features = data[6]
	features.replace(" ", "")
	features = features.split(',')
	count = 0
	up_feature = []
	for fat in features:
		if(fat != ""):
			up_feature.append(fat)
			count +=1
	features = list(up_feature)
	return (count)

def cutofffinal(id):
		outputfile = "static/"+str(id)+"/temp/cutoff.csv"
		f = open(outputfile,"w+")
		data = get_data_by_id(id)
		score = data[7]
		file = data[4]
		escalation_flag= data[11]
		features =feature_list(data[6])
		alert_flag = data[9]
		df = pd.read_csv(file)
		scoredata = df[score]
		temp = scoredata.to_numpy()
		alerts =0
		nonalerts = 0
		escalations = 0
		f.write("cut_off,alerts,nonalerts,escalations\n")
		for cutoff in range(1,100):
			for i in scoredata:
				if i < cutoff:
					nonalerts += 1
				else:
					alerts += 1

			
			for i in df.index:
				if df.at[i, escalation_flag] and df.at[i,score] < cutoff:
					df.at[i, escalation_flag] = 0

			escalated = df[df[escalation_flag] == 1]
			escs = len(escalated.index)
			f.write(str(cutoff) + ","+str(alerts)+ "," + str(nonalerts)+"," + str(escs) + "\n")
			nonalerts = 0
			alerts =0
		
		f.close()
def datatoshow(id):
	data =  get_data_by_id(id)
	file = data[4]
	df= pd.read_csv(file)
	df = df.head(50)
	cols  = df.columns
	for i in df.index:
		for col in cols:
			df.at[i,col] = round(df.at[i,col],3)
	df.to_csv("static/"+str(id)+"/temp/data50.csv")

def popup_data(id):
	data  = get_data_by_id(id)
	features = ""
	if(data[6]):
		features =feature_list(data[6])
	score = data[7]
	status = data[8]
	alert_flag = data[9]
	bunessAction = data[10]
	escalation_flag= data[11]
	return(features, score ,status ,alert_flag ,bunessAction,escalation_flag)

def cutofffile(id,cutoff):#testing done

	data = get_data_by_id(id)
	score = data[7]
	file = data[4]
	escalation_flag= data[11]
	features =feature_list(data[6])
	alert_flag = data[9]
	df = pd.read_csv(file)

	for i in df.index:
		if df.at[i,score] < cutoff:
		    df.at[i, alert_flag] = 0
		else:
		    df.at[i, alert_flag] = 1
		if df.at[i, escalation_flag] and df.at[i,score] < cutoff:
			df.at[i, escalation_flag] = 0
		

	arr = file.split(".")
	arr = arr[0].split("/")
	name  = "static/"+str(id)+"/temp/" + arr[-1] +"cutoff.csv"
	df.to_csv(name)
	alerted = df[df[alert_flag] == 1]
	escalated = alerted[alerted[escalation_flag] == 1]
	non_alerted = df[df[alert_flag] == 0]
	alertsdata = calculate_stats(alerted,features)
	nonalertsdata = calculate_stats(non_alerted,features)
	escsdata = calculate_stats(escalated,features)
	return(alertsdata,nonalertsdata,escsdata,len(alerted.index), len(non_alerted.index),len(escalated.index))


def getModelName(id):
	data = get_data_by_id(id)
	return(data[1])